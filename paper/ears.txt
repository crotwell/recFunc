
Automated receiver function processing

Introduction

The aim of this is article is twofold, first to introduce an automated processing system based on SOD, and second to give results and pitfalls from our experience with using this system for automated receiver function calculations. EARS, the earthscope automated receiver survey, aims to calculate bulk crustal properties for all stations within the IRIS DMC using receiver functions. To do this we have employed SOD, the standing order for data, a FISSURES/DHI based software package we have developed to do automated data retrieval and processing.

Because automated processing proceeds, by definition, without a large amount of human input and guidance, there are differences with traditional seismic processing. An automated system must rely on quantitative measures of quality as opposed to the seismologist's insight into what constitutes a bad earthquake or seismogram. However, the advantage of non-human driven processing is that the volume of data to be used can be substantially larger. Use of these larger data volumes can have quality implications as well, making use of stacking techniques and statistics instead of intuition and detailed analysis by a seismologist. If a seismic technique can be structured so it can be driven by SOD, then there is really no reason not to run it over as large a volume of data as possible.

We have to date processed all earthquakes with magnitude 5.5 or above for all stations within the IRIS POND. Results of the EARS project are preliminary, and much work is ongoing into discerning stations with reliable results from stations where the automated system produces values of dubious quality. This includes producing meaningful error estimates. The EARS processing system has all data available at http://www.seis.sc.edu/ears.

Automated Processing with SOD

SOD, the standing order for data, is a freely available Java based software package for automated retrieval and processing of seismic data. It if built on top of DHI, the data handling infrastructure, which is a CORBA based system to access network/channel information, event data and seismograms. To use SOD, a user sets up an XML configuration file, a recipe in SOD parlance, that specifies the types of earthquakes, channels and seismograms that she is interested in. The level of specification is fine grained and flexible, allowing a high degree of customization. In addition to data selection and rejection, SOD can also apply processing to the data as it is retrieved, saving much time and effort. 

Overview of SOD

The basic structure of SOD is of three arms, the event arm for selecting earthquakes of interest, the network arm for selecting networks, stations and channels and the waveform arm that combines the results of the two previous arms, creating event-channel pairs and are used to retrieve and process seismograms. Downloading instructions and more information about SOD can be found at http://www.seis.sc.edu/SOD. 

Once SOD has retrieved seismograms, it can apply a suite of processing steps including the built in processors such as filtering, mean removal, instrument correction, cutting and signal to noise. In addition, it has the ability for externally written processors to be incorporated into the system. This is what we have done with EARS, using the built in SOD functionality to preprocess the seismograms and then using a custom processor to create the receiver functions.

Method

The receiver functions and crustal estimates within the EARS system are calculated in a completely automatic fashion using the iterative deconvolution technique of XYZ and Ammon and the H-K stacking technique of Zhu and Kanamori. In addition, the stacking is done with the phase-weighted stacking technique of Schimmel and Paulssen.

Data Selection

The goals of the EARS project are to be as broad as possible with respect to usable data. Hence we have processed all broadband channels for all stations available from the IRIS DMC, including both temporary and permanent stations. This has given us at the present time about 1700 stations.

All events above magnitude 5.5 in the WHDF catalog from the NEIC are processed. For each of these events, receiver functions are calculated for all stations with broadband data between 30 and 100 degrees for which the P phase exists in the prem model. However, only events-station pairs for which the iterative deconvolution has a greater than 80 percent match are used in the resulting stacks.

Processing

A standard set of processing steps are taken with all event-station pairs. First a window from 120 seconds before the P to 180 seconds after the P is requested from the IRIS DMC. Any seismograms containing gaps, or for which there are not 3 components are rejected. We then calculate a simple signal to noise comparing the variance of a long time window from 105 to 5 seconds before the predicted P arrival with the variance of a short time window from 1 second before to 5 seconds after the predicted P arrival. This is not the same as traditional long to short term signal to noise ratios, but provides a simliar measure of the usefulness of the P arrival. Earthquakes for which the maximum signal to noise over the 3 components is less than 2 are rejected.

A cut operation is then performed to window the data to 30 seconds before the P to 120 seconds after the P in preparation for the deconvolution. The mean and trend are removed, a taper is applied and the overall gain from each channel's response is applied. We do not apply a full instrument deconvolution, instead assuming that the response is relatively flat in the passband of interest. The data is then filtered with a butterworth bandpass filter from 50 seconds
to 5 hertz.

The iterative deconvolution is then applied to the data. This is done with a gaussian width of 2.5. The iteration is limited to 400 steps or a percentage match of 99.99, which effectively means that all 400 steps are always applied. Both radial and transverse receiver functions are calculated and are then stored in the database regardless of the percentage match that they acheive.

Stacking

Because low percentage match receiver functions are often contaminated with noise or lack signal, we do not stack receiver functions for which the percentage match is below 80 percent. For some stations, especially within short-lived temporary deployments, this severly limits the number of receiver functions with the stacks, however, for an automated system this seems a
reasonable threshold. The stacking technique of Zhu and Kanamori is based on the idea of transfering the amplitudes of the receiver function from a time based system into a system based on crustal thickness (H) and Vp/Vs ratio, (K) by using the predicted times of the 3 principal crustal revibirations, Ps, PpPs and PsPs/PpSs. Once this transformation has been done, the amplitudes no longer have a time dependency, which eliminates the ray parameter as a variable. Thus, earthquakes at different distances, with correspondingly different ray
parameters can be stacked.

In addition to this stacking idea based on predicted arrival times, we also make use of phase-weights for the stacks. This is a non-linear stacking technique that weights the straight amplitude stack by the square of a sum of unit magnitude complex values constructed from the instantaeous phase of the receiver function. This technique greatly sharpens the stacks for coherant signals at the little expense of calculating the hilbert transform of the receiver functions.

Once the stacks are calculated, the thickness and Vp/Vs corresponding to the maximum value of the stack is taken to be the estimate of the bulk crustal properties. 



Comparision with Prior Results

Because of the automated nature of this study, a comparison with prior results
is of great interest. We make comparisons with 4 studies to understand how
well the automated system can do without resorting to a seismologist's
intuition for reasonable and unreasonable results.

Comparison with Zhu and Kanamori

This paper introduced the HK stacking method, and so a comparison with their results is appropriate.  

Comparison with Ristra

Comparison with Moma

Comparison with Crust2.0

Because Crust2.0 is a global crustal model, it is particuarly useful to compare with EARS as there is not a bias to a particular region. We compare our results with the closest data point within the Crust2.0 model as well as interpolating from the closest 4 data points.


