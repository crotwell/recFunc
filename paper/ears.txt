
Automated receiver function processing

Introduction

The aim of this is article is twofold, first to introduce an automated processing system based on SOD, and second to give results and pitfalls from our experience with using this system for automated receiver function calculations. EARS, the earthscope automated receiver survey, aims to calculate bulk crustal properties for all stations within the IRIS DMC using receiver functions. To do this we have employed SOD, the standing order for data, a FISSURES/DHI based software package we have developed to do automated data retrieval and processing.

Because automated processing proceeds, by definition, without a large amount of human input and guidance, there are differences with traditional seismic processing. An automated system must rely on quantitative measures of quality as opposed to the seismologist's insight into what constitutes a bad earthquake or seismogram. However, the advantage of non-human driven processing is that the volume of data to be used can be substantially larger. Use of these larger data volumes can have quality implications as well, making use of stacking techniques and statistics instead of intuition and detailed analysis by a seismologist. If a seismic technique can be structured so it can be driven by SOD, then there is really no reason not to run it over as large a volume of data as possible.

We have to date processed all earthquakes with magnitude 5.5 or above for all stations within the IRIS POND. Results of the EARS project are preliminary, and much work is ongoing into discerning stations with reliable results from stations where the automated system produces values of dubious quality. This includes producing meaningful error estimates. The EARS processing system has all data available at http://www.seis.sc.edu/ears.

Automated Processing with SOD

SOD, the standing order for data, is a freely available Java based software package for automated retrieval and processing of seismic data. It if built on top of DHI, the data handling infrastructure, which is a CORBA based system to access network/channel information, event data and seismograms. To use SOD, a user sets up an XML configuration file, a recipe in SOD parlance, that specifies the types of earthquakes, channels and seismograms that she is interested in. The level of specification is fine grained and flexible, allowing a high degree of customization. In addition to data selection and rejection, SOD can also apply processing to the data as it is retrieved, saving much time and effort. SOD is designed to be a long running background application, and can either mine historical data or wait for earthquakes to occur and process them as they occur. Downloading instructions and more information about SOD can be found at http://www.seis.sc.edu/SOD.

Overview of SOD

SOD's primary function is the automatic retrieval of seismograms. To retrieve a seismogram, it needs two basic pieces of information, a channel and a time window. The time window in SOD is based on an earthquake, or event, and so this naturally leads to a separation of the configuration into 3 sections, or arms, one for channels, one for events and a third for the combination of events and channels. 

The first is the network arm, which controls which networks, stations and channels are acceptable. There is considerable flexibility in deciding which networks, stations and channels are acceptable, based on codes, location, active time and distance from a location. In addition, each of these can be combined within logical ANDs and ORs to allow the creation of very complex criteria built up from simple pieces. In addition, there are channel processors that can do such tasks as retrieve the instrument response to be saved as a resp or sac polezero file.

The event arm for selects earthquakes of interest, based on location, magnitude, depth, catalog and time. Fine grained selection is possible, including logical operations, using each of these as well as distance, azimuth and back azimuth from a point and linear distance with magnitude from a point.

The waveform arm combines the results of the two previous arms, creating event-channel pairs and are used to retrieve and process seismograms. These event-channel pairs can also be selected based on properties like distance, azimuth, back azimuth, existence of phases, location of midpoints. The time window is determined based on phases and the event and channel, with further select based on the actual seismic data.

Once SOD has retrieved seismograms, it can apply a suite of processing steps including the built in processors such as filtering, mean removal, instrument correction, cutting and signal to noise. In addition, it has the ability for externally written processors to be incorporated into the system. This is what we have done with EARS, using the built in SOD functionality to preprocess the seismograms and then using a custom processor to create the receiver functions.

EARS

Method

The receiver functions and crustal estimates within the EARS system are calculated in a completely automatic fashion using the iterative deconvolution technique of XYZ and Ammon and the H-K stacking technique of Zhu and Kanamori. In addition, the stacking is done with the phase-weighted stacking technique of Schimmel and Paulssen.

Data Selection

The goals of the EARS project are to be as broad as possible with respect to usable data. Hence we have processed all broadband channels for all stations available from the IRIS DMC, including both temporary and permanent stations. This has given us at the present time about 1700 stations.

All events above magnitude 5.5 in the WHDF catalog from the NEIC are processed. For each of these events, receiver functions are calculated for all stations with broadband data between 30 and 100 degrees for which the P phase exists in the prem model. However, only events-station pairs for which the iterative deconvolution has a greater than 80 percent match are used in the resulting stacks.

Processing

A standard set of processing steps are taken with all event-station pairs. First a window from 120 seconds before the P to 180 seconds after the P is requested from the IRIS DMC. Any seismograms containing gaps, or for which there are not 3 components are rejected. We then calculate a simple signal to noise comparing the variance of a long time window from 105 to 5 seconds before the predicted P arrival with the variance of a short time window from 1 second before to 5 seconds after the predicted P arrival. This is not the same as traditional long to short term signal to noise ratios, but provides a simliar measure of the usefulness of the P arrival. Earthquakes for which the maximum signal to noise over the 3 components is less than 2 are rejected.

A cut operation is then performed to window the data to 30 seconds before the P to 120 seconds after the P in preparation for the deconvolution. The mean and trend are removed, a taper is applied and the overall gain from each channel's response is applied. We do not apply a full instrument deconvolution, instead assuming that the response is relatively flat in the passband of interest. The data is then filtered with a butterworth bandpass filter from 50 seconds
to 5 hertz.

The iterative deconvolution is then applied to the data. This is done with a gaussian width of 2.5. The iteration is limited to 400 steps or a percentage match of 99.99, which effectively means that all 400 steps are always applied. Both radial and transverse receiver functions are calculated and are then stored in the database regardless of the percentage match that they acheive.

Stacking

Because low percentage match receiver functions are often contaminated with noise or lack signal, we do not stack receiver functions for which the percentage match is below 80 percent. For some stations, especially within short-lived temporary deployments, this severly limits the number of receiver functions with the stacks, however, for an automated system this seems a
reasonable threshold. The stacking technique of Zhu and Kanamori is based on the idea of transfering the amplitudes of the receiver function from a time based system into a system based on crustal thickness (H) and Vp/Vs ratio, (K) by using the predicted times of the 3 principal crustal revibirations, Ps, PpPs and PsPs/PpSs. Once this transformation has been done, the amplitudes no longer have a time dependency, which eliminates the ray parameter as a variable. Thus, earthquakes at different distances, with correspondingly different ray
parameters can be stacked.

In addition to this stacking idea based on predicted arrival times, we also make use of phase-weights for the stacks. This is a non-linear stacking technique that weights the straight amplitude stack by the square of a sum of unit magnitude complex values constructed from the instantaeous phase of the receiver function. This technique greatly sharpens the stacks for coherant signals at the little expense of calculating the hilbert transform of the receiver functions.

Once the stacks are calculated, the thickness and Vp/Vs corresponding to the maximum value of the stack is taken to be the estimate of the bulk crustal properties. 

Error Estimation

Error estimation is especially import in an automated system, and we have tried to provide realistic errors by making use of a bootstrap technique. First we create a new set of receiver functions by randomly sampling with replacement from the original set. For example the origin set might contain receiver functions one to 10, and the sampled set might contain {3, 2, 9, 4, 2, 7, 4, 1, 1, 5}. The sampled set has the same size as the original, but may contain duplicates of some receiver functions. A stack is calculated from this new set and the global maximum is noted. The resampling procedure is repeated for some number of tries, 100 in our case, to produce a set of thickness and Vp/Vs values on which a variance can be calculated. We also tried an error estimate based on the curvature of the maxima, but found that this estimate often produced errors that are too small to be realistic. The other advantage of the resmpling technique is that a local maxima different from the global maxima might be the global maxima in some of the resamplings, effecting the error estimate in ways that the simple curvature estimate cannot.

Comparision with Prior Results

Because of the automated nature of this study, a comparison with prior results
is of great interest. We make comparisons with 4 studies to understand how
well the automated system can do without resorting to a seismologist's
intuition for reasonable and unreasonable results.

Comparison with Zhu and Kanamori

This paper introduced the HK stacking method, and so a comparison with their results is appropriate.  

Comparison with Ristra

Comparison with Moma

Comparison with Crust2.0

Because Crust2.0 is a global crustal model, it is particuarly useful to compare with EARS as there is not a bias to a particular region. We compare our results with the closest data point within the Crust2.0 model as well as interpolating from the closest 4 data points.

Problems and Pitfalls

The most obvious problem to be encountered with dealing with this volume of data is the shear volume of data, along with the computer time to process it. In the EARS case, the most costly processing step in terms of CPU time is the calculation of the receiver functions. In order to insulate ourselves from having to repeat this, we store the calculated receiver functions in a database, and then perform the stacking and analysis separately. This also has the advantage of allowing the use of multiple machines to parallelize the computations. We made use of a central PostgreSQL database with separate instances of SOD running on four to six Macintosh computers. Each separate run of SOD worked on either a different time interval, or a different network, allowing a simply, yet effect, sharing of the workload. An advantage of SOD is that it is written in Java and so is platform indenpendent, and so we could have used a mixed network had it been available.

Not as costly as the receiver function calculation, but still nontrivial is the bootstrap error estimation. We have addressed this in a similar way, buy spreading the resampling across multiple computers on our network.


